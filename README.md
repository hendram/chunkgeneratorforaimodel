# ğŸ›°ï¸ The Backend

This project is the **backend for the Auto Answering Q&A AI Agent Service**.  
It acts as the central hub, orchestrating message delivery between:

- ğŸ“¡ **Kafka** (messaging backbone)
- ğŸ¤– **Puppeteer Service**
- ğŸ•·ï¸ **Puppeteer Workers (1, 2, 3)**
- ğŸ§© **VectorEmbedGen (TiDB connector)**
- ğŸŒ **AutoAnsweringForm** (frontend)

Some Communication is powered by **Kafka**, ensuring reliable and scalable messaging between all components.  

---

## ğŸ“¦ About This Package

The backend works as the **control center** of the entire pipeline:  
- Direct connections to services like vectorembedgen.  
- Kafka-based messaging for workers and puppeteer service.  
- Handles message transformation, routing, and enrichment.  

Packages that interact with it:
- ğŸ” `puppeteerservice`  
- ğŸ•·ï¸ `puppeteerworker1`, `puppeteerworker2`, `puppeteerworker3`  
- ğŸ“‹ `autoansweringform`  
- ğŸ§© `vectorembedgen`  


---

## âš™ï¸ How It Works

The backend accepts messages from the **frontend** in two formats:

### ğŸ” Search Job

## âš™ï¸ How It Works

The backend accepts messages from the **frontend** in two formats:

### ğŸ” Search Job

For Example:

```bash
{
  "topic": {
    "searched": "tidb vector --filetype=html --filetype=xhtml --filetype=text",
    "searchEngine": "search.brave.com",
    "site": [
      "https://docs.google.com/forms/d/e/1FAIpQLSfRj7VFEAZJIm8HgE3lk0K_b5i9w0mgX9G2_XntzbptuURYiw/viewform?usp=dialog"
    ]
  }
}
```

or

###ğŸ¢ Corporate Knowledge Base Database Population Job

For Example:

```bash
topic: { corporate: "https://hendram.github.io/Knowledgebase/",
         topic: "mongodb rag"
}
```

## ğŸ”„ Processing Logic

### 1ï¸âƒ£ For **Search Jobs**

#### ğŸ“ Step 1: Initial Keyword Check
- ğŸ“¤ Message received from autoansweringform(frontend) is forwarded to **Puppeteer**.  
- ğŸ” Keywords are sent to **VectorEmbedGen** at `/insertsearchtodb` to check if a table already exists in **TiDB**.  
  - âœ… If exists â†’ continue.  
  - â• If not â†’ create the table, then continue.  

---

#### âœ‚ï¸ Step 2: Message Splitting
The original message is **split into two smaller jobs**:
- ğŸŸ£ `{ searched, searchEngine }`  
- ğŸ”µ `{ site }`  

ğŸ‘‰ These are sent to **Puppeteer** at **different times**.  

---

#### ğŸŒ Step 3: Scraping Search Results
1. The ğŸŸ£ `{ searched, searchEngine }` job is processed first.  

2. ğŸ”— Resulting links are **divided into 3 parts** â†’ sent to:  
   - ğŸ•·ï¸ `puppeteerworker1`  
   - ğŸ•·ï¸ `puppeteerworker2`  
   - ğŸ•·ï¸ `puppeteerworker3`  

3. After scraping:  
   - ğŸ§¹ **Merged** into one message  
   - ğŸª„ **Cleaned** (HTML tags removed)  
   - âœ‚ï¸ **Chunked**  
   - ğŸ“ **Vectorized**  
   - ğŸ’¾ Stored in **TiDB** via `/embed`  

#### ğŸ¢ Step 4: Scraping Target Site

1. ğŸ”µ The `{ site }` message is processed **after** the external links are finished.  
2. ğŸ–¥ï¸ Content is scraped by **Puppeteer**.  
3. ğŸ“¦ Results are then processed through several stages:  
   - ğŸ§¹ **Google Gemini LLM** â†’ sterilize and remove tags  
   - âœ‚ï¸ **Chunked** into smaller pieces  
   - ğŸ“ **Vectorized** for embedding  

4. âš–ï¸ Final embeddings are **compared** via `/searchvectordb` against:  
   - ğŸŸ¢ `_internal`  
   - ğŸ”µ `_external`  
   - ğŸ”€ Or both  

âœ… This ensures the **best possible Q&A results** are returned.  


#### 2ï¸âƒ£For Corporate Database Populate Jobs 

1. ğŸ”‘ A message with the **corporate key** just received from autoansweringform(frontend) is sent **directly** to Puppeteer.  
2. ğŸ–¥ï¸ Puppeteer scrapes the **content**.  
3. ğŸ”„ The content flows through these stages:  
   - ğŸ§¹ **cleanAndChunk** â†’ prepare the data  
   - ğŸ“ **vectorizedCorporate** â†’ generate corporate embeddings  
4. ğŸ“¡ Vector embeddings are sent to **VectorEmbedGen** via `/embedcorporate`.  
5. ğŸ—„ï¸ A **TiDB table** is created in the test database using the format: {table_name}_internal 
ğŸ‘‰ where **`table_name`** is automatically inferred from the userâ€™s topic inputted by user on Topic input form.  

âœ… This pipeline ensures corporate data is properly scraped, cleaned, embedded, and stored for **efficient retrieval**.  


---


### ğŸ”„ Message Exchange Details  


#### ğŸ–¥ï¸ Backend â‡„ ğŸ§  VectorEmbedGen  
- ğŸ“¡ Communication: **Direct fetch requests**  
- âš¡ Fast and lightweight for embedding operations  

---

#### ğŸ–¥ï¸ Backend â‡„ ğŸ¤– Puppeteer Service & Workers  
- ğŸ”Œ Communication: **Kafka**  
- â³ Why Kafka?  
  - Scraping takes a long time  
  - âŒ Fetch calls may **timeout**  
  - âœ… Kafka ensures **reliability & buffering**  

---

#### ğŸ–¥ï¸ Backend â‡„ ğŸ“ AutoAnsweringForm (Frontend)  
- ğŸ“¥ Accepts messages from frontend via **fetch**  
- ğŸ“¤ Sends responses back via **SSE (Server-Sent Events)**  
  - âš¡ Keeps the connection alive  
  - â³ Supports long-running processes  
  - âœ… Users get **real-time answers** without refreshing  

---


ğŸŒ **In summary:**  
- **Lightweight ops** â†’ fetch (Backend â‡„ VectorEmbedGen)  
- **Heavy ops** â†’ Kafka (Backend â‡„ Puppeteer Workers)  
- **User-facing ops** â†’ fetch + SSE (Backend â‡„ AutoAnsweringForm)  


---

âš¡ Platform Requirements

ğŸ’¾ At least 12 GB RAM

ğŸ–¥ï¸ At least 4 CPU cores (Intel i5 or higher recommended)

ğŸ³ Docker installed


---

ğŸš€ How to Run It


ğŸ“¥ Download

```bash
docker pull ghcr.io/hendram/chunkgeneratorforaimodel
```

â–¶ï¸ Start

```bash
docker run -it -d --network=host ghcr.io/hendram/chunkgeneratorforaimodel bash
```

ğŸ” Check Running Container

```bash
docker ps
```

```bash
CONTAINER ID   IMAGE                                         NAME                    STATUS
123abc456def   ghcr.io/hendram/chunkgeneratorforaimodel      practical_chatterjee    Up 5 minutes
```

ğŸ“¦ Enter Container

```bash
docker exec -it practical_chatterjee /bin/bash
```

ğŸƒ Run the Service

```bash
cd /home/chunkgeneratorforaimodel
node server.js
```


