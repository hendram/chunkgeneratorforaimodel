# üõ∞Ô∏è The Backend

This project is the **backend for the Auto Answering Q&A AI Agent Service**.  
It acts as the central hub, orchestrating message delivery between:

- üì° **Kafka** (messaging backbone)
- ü§ñ **Puppeteer Service**
- üï∑Ô∏è **Puppeteer Workers (1, 2, 3)**
- üß© **VectorEmbedGen (TiDB connector)**
- üåê **AutoAnsweringForm** (frontend)

Some Communication is powered by **Kafka**, ensuring reliable and scalable messaging between all components.  

---

## üì¶ About This Package

The backend works as the **control center** of the entire pipeline:  
- Direct connections to services like vectorembedgen.  
- Kafka-based messaging for workers and puppeteer service.  
- Handles message transformation, routing, and enrichment.  

Packages that interact with it:
- üîé `puppeteerservice`  
- üï∑Ô∏è `puppeteerworker1`, `puppeteerworker2`, `puppeteerworker3`  
- üìã `autoansweringform`  
- üß© `vectorembedgen`  


---

## ‚öôÔ∏è How It Works

The backend accepts messages from the **frontend** in two formats:

### üîç Search Job

## ‚öôÔ∏è How It Works

The backend accepts messages from the **frontend** in two formats:

### üîç Search Job

For Example:

```bash
{
  "topic": {
    "searched": "tidb vector --filetype=html --filetype=xhtml --filetype=text",
    "searchEngine": "search.brave.com",
    "site": [
      "https://docs.google.com/forms/d/e/1FAIpQLSfRj7VFEAZJIm8HgE3lk0K_b5i9w0mgX9G2_XntzbptuURYiw/viewform?usp=dialog"
    ]
  }
}
```

or

###üè¢ Corporate Knowledge Base Database Population Job

For Example:

```bash
topic: { corporate: "https://hendram.github.io/Knowledgebase/",
         topic: "mongodb rag"
}
```

## üîÑ Processing Logic

### 1Ô∏è‚É£ For **Search Jobs**

#### üìù Step 1: Initial Keyword Check
- üì§ Message received from autoansweringform(frontend) is forwarded to **Puppeteer**.  
- üîç Keywords are sent to **VectorEmbedGen** at `/insertsearchtodb` to check if a table already exists in **TiDB**.  
  - ‚úÖ If exists ‚Üí continue.  
  - ‚ûï If not ‚Üí create the table, then continue.  

---

#### ‚úÇÔ∏è Step 2: Message Splitting
The original message is **split into two smaller jobs**:
- üü£ `{ searched, searchEngine }`  
- üîµ `{ site }`  

üëâ These are sent to **Puppeteer** at **different times**.  

---

#### üåê Step 3: Scraping Search Results
1. The üü£ `{ searched, searchEngine }` job is processed first.  

2. üîó Resulting links are **divided into 3 parts** ‚Üí sent to:  
   - üï∑Ô∏è `puppeteerworker1`  
   - üï∑Ô∏è `puppeteerworker2`  
   - üï∑Ô∏è `puppeteerworker3`  

3. After scraping:  
   - üßπ **Merged** into one message  
   - ü™Ñ **Cleaned** (HTML tags removed)  
   - ‚úÇÔ∏è **Chunked**  
   - üìê **Vectorized**  
   - üíæ Stored in **TiDB** via `/embed`  

#### üè¢ Step 4: Scraping Target Site

1. üîµ The `{ site }` message is processed **after** the external links are finished.  
2. üñ•Ô∏è Content is scraped by **Puppeteer**.  
3. üì¶ Results are then processed through several stages:  
   - üßπ **Google Gemini LLM** ‚Üí sterilize and remove tags  
   - ‚úÇÔ∏è **Chunked** into smaller pieces  
   - üìê **Vectorized** for embedding  

4. ‚öñÔ∏è Final embeddings are **compared** via `/searchvectordb` against:  
   - üü¢ `_internal`  
   - üîµ `_external`  
   - üîÄ Or both  

‚úÖ This ensures the **best possible Q&A results** are returned.  


#### 2Ô∏è‚É£For Corporate Database Populate Jobs 

1. üîë A message with the **corporate key** just received from autoansweringform(frontend) is sent **directly** to Puppeteer.  
2. üñ•Ô∏è Puppeteer scrapes the **content**.  
3. üîÑ The content flows through these stages:  
   - üßπ **cleanAndChunk** ‚Üí prepare the data  
   - üìê **vectorizedCorporate** ‚Üí generate corporate embeddings  
4. üì° Vector embeddings are sent to **VectorEmbedGen** via `/embedcorporate`.  
5. üóÑÔ∏è A **TiDB table** is created in the test database using the format: {table_name}_internal 
üëâ where **`table_name`** is automatically inferred from the user‚Äôs topic inputted by user on Topic input form.  

‚úÖ This pipeline ensures corporate data is properly scraped, cleaned, embedded, and stored for **efficient retrieval**.  


---


### üîÑ Message Exchange Details  


#### üñ•Ô∏è Backend ‚áÑ üß† VectorEmbedGen  
- üì° Communication: **Direct fetch requests**  
- ‚ö° Fast and lightweight for embedding operations  

---

#### üñ•Ô∏è Backend ‚áÑ ü§ñ Puppeteer Service & Workers  
- üîå Communication: **Kafka**  
- ‚è≥ Why Kafka?  
  - Scraping takes a long time  
  - ‚ùå Fetch calls may **timeout**  
  - ‚úÖ Kafka ensures **reliability & buffering**  

---

#### üñ•Ô∏è Backend ‚áÑ üìù AutoAnsweringForm (Frontend)  
- üì• Accepts messages from frontend via **fetch**  
- üì§ Sends responses back via **SSE (Server-Sent Events)**  
  - ‚ö° Keeps the connection alive  
  - ‚è≥ Supports long-running processes  
  - ‚úÖ Users get **real-time answers** without refreshing  

---


üåê **In summary:**  
- **Lightweight ops** ‚Üí fetch (Backend ‚áÑ VectorEmbedGen)  
- **Heavy ops** ‚Üí Kafka (Backend ‚áÑ Puppeteer Workers)  
- **User-facing ops** ‚Üí fetch + SSE (Backend ‚áÑ AutoAnsweringForm)  


---

## ‚ö° Platform Requirements

üíæ At least 12 GB RAM

üñ•Ô∏è At least 4 CPU cores (Intel i5 or higher recommended)

üê≥ Docker installed


---

## üöÄ How to Run It


#### üì• Download

```bash
docker pull ghcr.io/hendram/chunkgeneratorforaimodel
```

#### ‚ñ∂Ô∏è Start

```bash
docker run -it -d --network=host ghcr.io/hendram/chunkgeneratorforaimodel bash
```

#### üîç Check Running Container

```bash
docker ps
```

```bash
CONTAINER ID   IMAGE                                         NAME                    STATUS
123abc456def   ghcr.io/hendram/chunkgeneratorforaimodel      practical_chatterjee    Up 5 minutes
```

#### üì¶ Enter Container

```bash
docker exec -it practical_chatterjee /bin/bash
```

#### üèÉ Run the Service

```bash
cd /home/chunkgeneratorforaimodel
node server.js
```

# üñ•Ô∏è  Code Overview

####  server.js

This Node.js server handles real-time streaming, Kafka messaging, and Puppeteer job dispatching. It also integrates with a FastAPI endpoint for database insertion.

---

## ‚ö° Technologies Used

- **Node.js** + **Express**: Server framework
- **Kafka**: Message queue for async processing
- **Puppeteer**: Headless browser automation
- **EventBus**: Internal event management
- **CORS**: Cross-origin request handling
- **Fetch API**: Calling external FastAPI endpoint

---

## üöÄ Key Features

###  1Ô∏è‚É£ CORS Middleware

```bash
app.use((req, res, next) => {
  if (applyCors(req, res)) return; // handles preflight requests
  next();
});
```

###  2Ô∏è‚É£ Streaming Endpoint

```bash
app.get("/stream", (req, res) => {
  registerStream(req, res);
});
```

  Registers a streaming connection for clients.

###  3Ô∏è‚É£ Job Submission (/search)

```bash
app.post("/search", async (req, res) => { ... });
```

  Handles topic submissions and routes jobs:


###  ‚úÖ Corporate Job Dispatch:

If topic.corporate exists, send to Puppeteer via Kafka.

####  ‚ö†Ô∏è Empty Corporate Topic:

Responds with No corporate topic.

####  üîç Searched Keyword:

Calls FastAPI insertsearchtodb, handles response, and sends relevant messages to Kafka.

####  ‚ùå Missing Keyword:

Responds with No searched keyword.

Internal Flow:

Log topic variables with timestamp.

Call FastAPI for vectorized search insertion.

Depending on FastAPI response:

Send only site info if answer === "yes".

Otherwise, send full topic, then optionally send site info after statusChanged event.

###  4Ô∏è‚É£ Kafka Consumers

```bash
await startConsumer();
await startPuppeteerConsumer();
```

Starts Kafka consumers on server boot:

startConsumer: General processing

startPuppeteerConsumer: Puppeteer job processing

###  5Ô∏è‚É£ Server Start

```bash
const PORT = process.env.PORT || 3000;

app.listen(PORT, async () => { ... });
```

Server listens on port 3000 (default)

Initializes Kafka consumers at startup.

####  üìù Utility Functions

Logging

```bash
function logTopicVariable(topic) {
  const now = new Date();
  const ts = now.toISOString().replace("T", " ").replace("Z", "");
  const ms = String(now.getMilliseconds()).padStart(3, "0");
  console.log(`[${ts},${ms}] INFO Sending request to insertsearchtodb ${JSON.stringify(topic)}`);
}
```

Logs each topic with timestamp + milliseconds.

Helps track database insertion requests.

#### üîó Flow Diagram

```bash
Client -> /search POST -> Validate topic
         ‚îú‚îÄ corporate exists -> Kafka -> Puppeteer
         ‚îî‚îÄ searched keyword -> FastAPI -> Kafka -> EventBus
```

# üåä Stream Module (`/lib/stream.js`)

This module handles **Server-Sent Events (SSE)**, allowing the server to push real-time updates to connected clients.

---

## ‚ö° Key Features

###  1Ô∏è‚É£ Connected Clients

```bash
const clients = [];
```
Maintains an array of all currently connected SSE clients.

###  2Ô∏è‚É£ Structured Logger
```bash
function logSSE(event, details = {}) {
  console.log(JSON.stringify({
    level: "INFO",
    timestamp: new Date().toISOString(),
    logger: "sse",
    event,
    ...details
  }));
}
```

Logs all SSE activity in JSON format.

Includes event type, timestamp, and additional details.

Useful for debugging and monitoring live streams.

###  3Ô∏è‚É£ Broadcast Results

```bash
export function handleResultToStream(allBest) { ... }
```

Sends a JSON payload (allBest) to all connected clients.

Logs:

clients_count: Number of connected clients

payload_size: Size of payload in bytes

Sends a heartbeat comment (:heartbeat) to clients every 5 seconds to keep connections alive.


###  4Ô∏è‚É£ Register New Client

```bash
export function registerStream(req, res) { ... }
```

Sets SSE headers:

Content-Type: text/event-stream

Cache-Control: no-cache

Connection: keep-alive

Adds the client res to the clients array.

Logs:

client_connected with current client count

Handles client disconnects:

Removes disconnected clients

Logs client_disconnected event

###  5Ô∏è‚É£ Global Heartbeat

```bash
setInterval(() => {
  clients.forEach((res) => res.write(":heartbeat\n\n"));
  if (clients.length > 0) {
    logSSE("heartbeat", { clients_count: clients.length });
  }
}, 10000);
```

Sends a global heartbeat every 10 seconds to all clients.

Keeps SSE connections alive.

Logs heartbeat events only if clients are connected.

#### üìù Usage

Register a new client:

```bash
app.get("/stream", (req, res) => {
  registerStream(req, res);
});
```
Push results to clients:

```bash
handleResultToStream(allBestArray);
```

allBestArray is any array of results you want to broadcast.

