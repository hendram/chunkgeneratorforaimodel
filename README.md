# ðŸ›°ï¸ The Backend

This project is the **backend for the Auto Answering Q&A AI Agent Service**.  
It acts as the central hub, orchestrating message delivery between:

- ðŸ“¡ **Kafka** (messaging backbone)
- ðŸ¤– **Puppeteer Service**
- ðŸ•·ï¸ **Puppeteer Workers (1, 2, 3)**
- ðŸ§© **VectorEmbedGen (TiDB connector)**
- ðŸŒ **AutoAnsweringForm** (frontend)

Some Communication is powered by **Kafka**, ensuring reliable and scalable messaging between all components.  

---

## ðŸ“¦ About This Package

The backend works as the **control center** of the entire pipeline:  
- Direct connections to services like vectorembedgen.  
- Kafka-based messaging for workers and puppeteer service.  
- Handles message transformation, routing, and enrichment.  

Packages that interact with it:
- ðŸ”Ž `puppeteerservice`  
- ðŸ•·ï¸ `puppeteerworker1`, `puppeteerworker2`, `puppeteerworker3`  
- ðŸ“‹ `autoansweringform`  
- ðŸ§© `vectorembedgen`  


---

## âš™ï¸ How It Works

The backend accepts messages from the **frontend** in two formats:

### ðŸ” Search Job

## âš™ï¸ How It Works

The backend accepts messages from the **frontend** in two formats:

### ðŸ” Search Job

For Example:

```bash
{
  "topic": {
    "searched": "tidb vector --filetype=html --filetype=xhtml --filetype=text",
    "searchEngine": "search.brave.com",
    "site": [
      "https://docs.google.com/forms/d/e/1FAIpQLSfRj7VFEAZJIm8HgE3lk0K_b5i9w0mgX9G2_XntzbptuURYiw/viewform?usp=dialog"
    ]
  }
}
```

or

###ðŸ¢ Corporate Knowledge Base Database Population Job

For Example:

```bash
topic: { corporate: "https://hendram.github.io/Knowledgebase/",
         topic: "mongodb rag"
}
```

## ðŸ”„ Processing Logic

### 1ï¸âƒ£ For **Search Jobs**

#### ðŸ“ Step 1: Initial Keyword Check
- ðŸ“¤ Message received from autoansweringform(frontend) is forwarded to **Puppeteer**.  
- ðŸ” Keywords are sent to **VectorEmbedGen** at `/insertsearchtodb` to check if a table already exists in **TiDB**.  
  - âœ… If exists â†’ continue.  
  - âž• If not â†’ create the table, then continue.  

---

#### âœ‚ï¸ Step 2: Message Splitting
The original message is **split into two smaller jobs**:
- ðŸŸ£ `{ searched, searchEngine }`  
- ðŸ”µ `{ site }`  

ðŸ‘‰ These are sent to **Puppeteer** at **different times**.  

---

#### ðŸŒ Step 3: Scraping Search Results
1. The ðŸŸ£ `{ searched, searchEngine }` job is processed first.  

2. ðŸ”— Resulting links are **divided into 3 parts** â†’ sent to:  
   - ðŸ•·ï¸ `puppeteerworker1`  
   - ðŸ•·ï¸ `puppeteerworker2`  
   - ðŸ•·ï¸ `puppeteerworker3`  

3. After scraping:  
   - ðŸ§¹ **Merged** into one message  
   - ðŸª„ **Cleaned** (HTML tags removed)  
   - âœ‚ï¸ **Chunked**  
   - ðŸ“ **Vectorized**  
   - ðŸ’¾ Stored in **TiDB** via `/embed`  

#### ðŸ¢ Step 4: Scraping Target Site

1. ðŸ”µ The `{ site }` message is processed **after** the external links are finished.  
2. ðŸ–¥ï¸ Content is scraped by **Puppeteer**.  
3. ðŸ“¦ Results are then processed through several stages:  
   - ðŸ§¹ **Google Gemini LLM** â†’ sterilize and remove tags  
   - âœ‚ï¸ **Chunked** into smaller pieces  
   - ðŸ“ **Vectorized** for embedding  

4. âš–ï¸ Final embeddings are **compared** via `/searchvectordb` against:  
   - ðŸŸ¢ `_internal`  
   - ðŸ”µ `_external`  
   - ðŸ”€ Or both  

âœ… This ensures the **best possible Q&A results** are returned.  


#### 2ï¸âƒ£For Corporate Database Populate Jobs 

1. ðŸ”‘ A message with the **corporate key** just received from autoansweringform(frontend) is sent **directly** to Puppeteer.  
2. ðŸ–¥ï¸ Puppeteer scrapes the **content**.  
3. ðŸ”„ The content flows through these stages:  
   - ðŸ§¹ **cleanAndChunk** â†’ prepare the data  
   - ðŸ“ **vectorizedCorporate** â†’ generate corporate embeddings  
4. ðŸ“¡ Vector embeddings are sent to **VectorEmbedGen** via `/embedcorporate`.  
5. ðŸ—„ï¸ A **TiDB table** is created in the test database using the format: {table_name}_internal 
ðŸ‘‰ where **`table_name`** is automatically inferred from the userâ€™s topic inputted by user on Topic input form.  

âœ… This pipeline ensures corporate data is properly scraped, cleaned, embedded, and stored for **efficient retrieval**.  


---


### ðŸ”„ Message Exchange Details  


#### ðŸ–¥ï¸ Backend â‡„ ðŸ§  VectorEmbedGen  
- ðŸ“¡ Communication: **Direct fetch requests**  
- âš¡ Fast and lightweight for embedding operations  

---

#### ðŸ–¥ï¸ Backend â‡„ ðŸ¤– Puppeteer Service & Workers  
- ðŸ”Œ Communication: **Kafka**  
- â³ Why Kafka?  
  - Scraping takes a long time  
  - âŒ Fetch calls may **timeout**  
  - âœ… Kafka ensures **reliability & buffering**  

---

#### ðŸ–¥ï¸ Backend â‡„ ðŸ“ AutoAnsweringForm (Frontend)  
- ðŸ“¥ Accepts messages from frontend via **fetch**  
- ðŸ“¤ Sends responses back via **SSE (Server-Sent Events)**  
  - âš¡ Keeps the connection alive  
  - â³ Supports long-running processes  
  - âœ… Users get **real-time answers** without refreshing  

---


ðŸŒ **In summary:**  
- **Lightweight ops** â†’ fetch (Backend â‡„ VectorEmbedGen)  
- **Heavy ops** â†’ Kafka (Backend â‡„ Puppeteer Workers)  
- **User-facing ops** â†’ fetch + SSE (Backend â‡„ AutoAnsweringForm)  


---

## âš¡ Platform Requirements

ðŸ’¾ At least 12 GB RAM

ðŸ–¥ï¸ At least 4 CPU cores (Intel i5 or higher recommended)

ðŸ³ Docker installed


---

## ðŸš€ How to Run It


#### ðŸ“¥ Download

```bash
docker pull ghcr.io/hendram/chunkgeneratorforaimodel
```

#### â–¶ï¸ Start

```bash
docker run -it -d --network=host ghcr.io/hendram/chunkgeneratorforaimodel bash
```

#### ðŸ” Check Running Container

```bash
docker ps
```

```bash
CONTAINER ID   IMAGE                                         NAME                    STATUS
123abc456def   ghcr.io/hendram/chunkgeneratorforaimodel      practical_chatterjee    Up 5 minutes
```

#### ðŸ“¦ Enter Container

```bash
docker exec -it practical_chatterjee /bin/bash
```

#### ðŸƒ Run the Service

```bash
cd /home/chunkgeneratorforaimodel
node server.js
```

# ðŸ–¥ï¸  Code Overview

####  server.js

This Node.js server handles real-time streaming, Kafka messaging, and Puppeteer job dispatching. It also integrates with a FastAPI endpoint for database insertion.

---

## âš¡ Technologies Used

- **Node.js** + **Express**: Server framework
- **Kafka**: Message queue for async processing
- **Puppeteer**: Headless browser automation
- **EventBus**: Internal event management
- **CORS**: Cross-origin request handling
- **Fetch API**: Calling external FastAPI endpoint

---

## ðŸš€ Key Features

###  1ï¸âƒ£ CORS Middleware

```bash
app.use((req, res, next) => {
  if (applyCors(req, res)) return; // handles preflight requests
  next();
});
```

###  2ï¸âƒ£ Streaming Endpoint

```bash
app.get("/stream", (req, res) => {
  registerStream(req, res);
});
```

  Registers a streaming connection for clients.

###  3ï¸âƒ£ Job Submission (/search)

```bash
app.post("/search", async (req, res) => { ... });
```

  Handles topic submissions and routes jobs:


###  âœ… Corporate Job Dispatch:

If topic.corporate exists, send to Puppeteer via Kafka.

####  âš ï¸ Empty Corporate Topic:

Responds with No corporate topic.

####  ðŸ” Searched Keyword:

Calls FastAPI insertsearchtodb, handles response, and sends relevant messages to Kafka.

####  âŒ Missing Keyword:

Responds with No searched keyword.

Internal Flow:

Log topic variables with timestamp.

Call FastAPI for vectorized search insertion.

Depending on FastAPI response:

Send only site info if answer === "yes".

Otherwise, send full topic, then optionally send site info after statusChanged event.

###  4ï¸âƒ£ Kafka Consumers

```bash
await startConsumer();
await startPuppeteerConsumer();
```

Starts Kafka consumers on server boot:

startConsumer: General processing

startPuppeteerConsumer: Puppeteer job processing

###  5ï¸âƒ£ Server Start

```bash
const PORT = process.env.PORT || 3000;

app.listen(PORT, async () => { ... });
```

Server listens on port 3000 (default)

Initializes Kafka consumers at startup.

####  ðŸ“ Utility Functions

Logging

```bash
function logTopicVariable(topic) {
  const now = new Date();
  const ts = now.toISOString().replace("T", " ").replace("Z", "");
  const ms = String(now.getMilliseconds()).padStart(3, "0");
  console.log(`[${ts},${ms}] INFO Sending request to insertsearchtodb ${JSON.stringify(topic)}`);
}
```

Logs each topic with timestamp + milliseconds.

Helps track database insertion requests.

#### ðŸ”— Flow Diagram

```bash
Client -> /search POST -> Validate topic
         â”œâ”€ corporate exists -> Kafka -> Puppeteer
         â””â”€ searched keyword -> FastAPI -> Kafka -> EventBus
```

# ðŸŒŠ Stream Module (`/lib/stream.js`)

This module handles **Server-Sent Events (SSE)**, allowing the server to push real-time updates to connected clients.

---

## âš¡ Key Features

###  1ï¸âƒ£ Connected Clients

```bash
const clients = [];
```
Maintains an array of all currently connected SSE clients.

###  2ï¸âƒ£ Structured Logger
```bash
function logSSE(event, details = {}) {
  console.log(JSON.stringify({
    level: "INFO",
    timestamp: new Date().toISOString(),
    logger: "sse",
    event,
    ...details
  }));
}
```

Logs all SSE activity in JSON format.

Includes event type, timestamp, and additional details.

Useful for debugging and monitoring live streams.

###  3ï¸âƒ£ Broadcast Results

```bash
export function handleResultToStream(allBest) { ... }
```

Sends a JSON payload (allBest) to all connected clients.

Logs:

clients_count: Number of connected clients

payload_size: Size of payload in bytes

Sends a heartbeat comment (:heartbeat) to clients every 5 seconds to keep connections alive.


###  4ï¸âƒ£ Register New Client

```bash
export function registerStream(req, res) { ... }
```

Sets SSE headers:

Content-Type: text/event-stream

Cache-Control: no-cache

Connection: keep-alive

Adds the client res to the clients array.

Logs:

client_connected with current client count

Handles client disconnects:

Removes disconnected clients

Logs client_disconnected event

###  5ï¸âƒ£ Global Heartbeat

```bash
setInterval(() => {
  clients.forEach((res) => res.write(":heartbeat\n\n"));
  if (clients.length > 0) {
    logSSE("heartbeat", { clients_count: clients.length });
  }
}, 10000);
```

Sends a global heartbeat every 10 seconds to all clients.

Keeps SSE connections alive.

Logs heartbeat events only if clients are connected.

#### ðŸ“ Usage

Register a new client:

```bash
app.get("/stream", (req, res) => {
  registerStream(req, res);
});
```
Push results to clients:

```bash
handleResultToStream(allBestArray);
```

allBestArray is any array of results you want to broadcast.

