# üõ∞Ô∏è The Backend

This project is the **backend for the Auto Answering Q&A AI Agent Service**.  
It acts as the central hub, orchestrating message delivery between:

- üì° **Kafka** (messaging backbone)
- ü§ñ **Puppeteer Service**
- üï∑Ô∏è **Puppeteer Workers (1, 2, 3)**
- üß© **VectorEmbedGen (TiDB connector)**
- üåê **AutoAnsweringForm** (frontend)

Some Communication is powered by **Kafka**, ensuring reliable and scalable messaging between all components.  

---

## üì¶ About This Package

The backend works as the **control center** of the entire pipeline:  
- Direct connections to services like vectorembedgen.  
- Kafka-based messaging for workers and puppeteer service.  
- Handles message transformation, routing, and enrichment.  

Packages that interact with it:
- üîé `puppeteerservice`  
- üï∑Ô∏è `puppeteerworker1`, `puppeteerworker2`, `puppeteerworker3`  
- üìã `autoansweringform`  
- üß© `vectorembedgen`  


---

## ‚öôÔ∏è How It Works

The backend accepts messages from the **frontend** in two formats:

### üîç Search Job

## ‚öôÔ∏è How It Works

The backend accepts messages from the **frontend** in two formats:

### üîç Search Job

For Example:

```bash
{
  "topic": {
    "searched": "tidb vector --filetype=html --filetype=xhtml --filetype=text",
    "searchEngine": "search.brave.com",
    "site": [
      "https://docs.google.com/forms/d/e/1FAIpQLSfRj7VFEAZJIm8HgE3lk0K_b5i9w0mgX9G2_XntzbptuURYiw/viewform?usp=dialog"
    ]
  }
}
```

or

###üè¢ Corporate Knowledge Base Database Population Job

For Example:

```bash
topic: { corporate: "https://hendram.github.io/Knowledgebase/",
         topic: "mongodb rag"
}
```

## üîÑ Processing Logic

### 1Ô∏è‚É£ For **Search Jobs**

#### üìù Step 1: Initial Keyword Check
- üì§ Message received from autoansweringform(frontend) is forwarded to **Puppeteer**.  
- üîç Keywords are sent to **VectorEmbedGen** at `/insertsearchtodb` to check if a table already exists in **TiDB**.  
  - ‚úÖ If exists ‚Üí continue.  
  - ‚ûï If not ‚Üí create the table, then continue.  

---

#### ‚úÇÔ∏è Step 2: Message Splitting
The original message is **split into two smaller jobs**:
- üü£ `{ searched, searchEngine }`  
- üîµ `{ site }`  

üëâ These are sent to **Puppeteer** at **different times**.  

---

#### üåê Step 3: Scraping Search Results
1. The üü£ `{ searched, searchEngine }` job is processed first.  

2. üîó Resulting links are **divided into 3 parts** ‚Üí sent to:  
   - üï∑Ô∏è `puppeteerworker1`  
   - üï∑Ô∏è `puppeteerworker2`  
   - üï∑Ô∏è `puppeteerworker3`  

3. After scraping:  
   - üßπ **Merged** into one message  
   - ü™Ñ **Cleaned** (HTML tags removed)  
   - ‚úÇÔ∏è **Chunked**  
   - üìê **Vectorized**  
   - üíæ Stored in **TiDB** via `/embed`  

#### üè¢ Step 4: Scraping Target Site

1. üîµ The `{ site }` message is processed **after** the external links are finished.  
2. üñ•Ô∏è Content is scraped by **Puppeteer**.  
3. üì¶ Results are then processed through several stages:  
   - üßπ **Google Gemini LLM** ‚Üí sterilize and remove tags  
   - ‚úÇÔ∏è **Chunked** into smaller pieces  
   - üìê **Vectorized** for embedding  

4. ‚öñÔ∏è Final embeddings are **compared** via `/searchvectordb` against:  
   - üü¢ `_internal`  
   - üîµ `_external`  
   - üîÄ Or both  

‚úÖ This ensures the **best possible Q&A results** are returned.  


#### 2Ô∏è‚É£For Corporate Database Populate Jobs 

1. üîë A message with the **corporate key** just received from autoansweringform(frontend) is sent **directly** to Puppeteer.  
2. üñ•Ô∏è Puppeteer scrapes the **content**.  
3. üîÑ The content flows through these stages:  
   - üßπ **cleanAndChunk** ‚Üí prepare the data  
   - üìê **vectorizedCorporate** ‚Üí generate corporate embeddings  
4. üì° Vector embeddings are sent to **VectorEmbedGen** via `/embedcorporate`.  
5. üóÑÔ∏è A **TiDB table** is created in the test database using the format: {table_name}_internal 
üëâ where **`table_name`** is automatically inferred from the user‚Äôs topic inputted by user on Topic input form.  

‚úÖ This pipeline ensures corporate data is properly scraped, cleaned, embedded, and stored for **efficient retrieval**.  


---


### üîÑ Message Exchange Details  


#### üñ•Ô∏è Backend ‚áÑ üß† VectorEmbedGen  
- üì° Communication: **Direct fetch requests**  
- ‚ö° Fast and lightweight for embedding operations  

---

#### üñ•Ô∏è Backend ‚áÑ ü§ñ Puppeteer Service & Workers  
- üîå Communication: **Kafka**  
- ‚è≥ Why Kafka?  
  - Scraping takes a long time  
  - ‚ùå Fetch calls may **timeout**  
  - ‚úÖ Kafka ensures **reliability & buffering**  

---

#### üñ•Ô∏è Backend ‚áÑ üìù AutoAnsweringForm (Frontend)  
- üì• Accepts messages from frontend via **fetch**  
- üì§ Sends responses back via **SSE (Server-Sent Events)**  
  - ‚ö° Keeps the connection alive  
  - ‚è≥ Supports long-running processes  
  - ‚úÖ Users get **real-time answers** without refreshing  

---


üåê **In summary:**  
- **Lightweight ops** ‚Üí fetch (Backend ‚áÑ VectorEmbedGen)  
- **Heavy ops** ‚Üí Kafka (Backend ‚áÑ Puppeteer Workers)  
- **User-facing ops** ‚Üí fetch + SSE (Backend ‚áÑ AutoAnsweringForm)  


---

## ‚ö° Platform Requirements

üíæ At least 12 GB RAM

üñ•Ô∏è At least 4 CPU cores (Intel i5 or higher recommended)

üê≥ Docker installed


---

## üöÄ How to Run It


#### üì• Download

```bash
docker pull ghcr.io/hendram/chunkgeneratorforaimodel
```

#### ‚ñ∂Ô∏è Start

```bash
docker run -it -d --network=host ghcr.io/hendram/chunkgeneratorforaimodel bash
```

#### üîç Check Running Container

```bash
docker ps
```

```bash
CONTAINER ID   IMAGE                                         NAME                    STATUS
123abc456def   ghcr.io/hendram/chunkgeneratorforaimodel      practical_chatterjee    Up 5 minutes
```

#### üì¶ Enter Container

```bash
docker exec -it practical_chatterjee /bin/bash
```

#### üèÉ Run the Service

```bash
cd /home/chunkgeneratorforaimodel
node server.js
```

# üñ•Ô∏è  Code Overview

####  server.js

This Node.js server handles real-time streaming, Kafka messaging, and Puppeteer job dispatching. It also integrates with a FastAPI endpoint for database insertion.

---

## ‚ö° Technologies Used

- **Node.js** + **Express**: Server framework
- **Kafka**: Message queue for async processing
- **Puppeteer**: Headless browser automation
- **EventBus**: Internal event management
- **CORS**: Cross-origin request handling
- **Fetch API**: Calling external FastAPI endpoint

---

## üöÄ Key Features

###  1Ô∏è‚É£ CORS Middleware

```bash
app.use((req, res, next) => {
  if (applyCors(req, res)) return; // handles preflight requests
  next();
});
```

###  2Ô∏è‚É£ Streaming Endpoint

```bash
app.get("/stream", (req, res) => {
  registerStream(req, res);
});
```

  Registers a streaming connection for clients.

###  3Ô∏è‚É£ Job Submission (/search)

```bash
app.post("/search", async (req, res) => { ... });
```

  Handles topic submissions and routes jobs:


###  ‚úÖ Corporate Job Dispatch:

If topic.corporate exists, send to Puppeteer via Kafka.

####  ‚ö†Ô∏è Empty Corporate Topic:

Responds with No corporate topic.

####  üîç Searched Keyword:

Calls FastAPI insertsearchtodb, handles response, and sends relevant messages to Kafka.

####  ‚ùå Missing Keyword:

Responds with No searched keyword.

####  Internal Flow:

Log topic variables with timestamp.

Call FastAPI for vectorized search insertion.

Depending on FastAPI response:

Send only site info if answer === "yes".

Otherwise, send full topic, then optionally send site info after statusChanged event.

###  4Ô∏è‚É£ Kafka Consumers

```bash
await startConsumer();
await startPuppeteerConsumer();
```

Starts Kafka consumers on server boot:

startConsumer: General processing

startPuppeteerConsumer: Puppeteer job processing

###  5Ô∏è‚É£ Server Start

```bash
const PORT = process.env.PORT || 3000;

app.listen(PORT, async () => { ... });
```

Server listens on port 3000 (default)

Initializes Kafka consumers at startup.

####  üìù Utility Functions

Logging

```bash
function logTopicVariable(topic) {
  const now = new Date();
  const ts = now.toISOString().replace("T", " ").replace("Z", "");
  const ms = String(now.getMilliseconds()).padStart(3, "0");
  console.log(`[${ts},${ms}] INFO Sending request to insertsearchtodb ${JSON.stringify(topic)}`);
}
```

Logs each topic with timestamp + milliseconds.

Helps track database insertion requests.

#### üîó Flow Diagram

```bash
Client -> /search POST -> Validate topic
         ‚îú‚îÄ corporate exists -> Kafka -> Puppeteer
         ‚îî‚îÄ searched keyword -> FastAPI -> Kafka -> EventBus
```

---

# üåä Stream Module (`/lib/stream.js`)

This module handles **Server-Sent Events (SSE)**, allowing the server to push real-time updates to connected clients.

---

## ‚ö° Key Features

###  1Ô∏è‚É£ Connected Clients

```bash
const clients = [];
```
Maintains an array of all currently connected SSE clients.

###  2Ô∏è‚É£ Structured Logger
```bash
function logSSE(event, details = {}) {
  console.log(JSON.stringify({
    level: "INFO",
    timestamp: new Date().toISOString(),
    logger: "sse",
    event,
    ...details
  }));
}
```

Logs all SSE activity in JSON format.

Includes event type, timestamp, and additional details.

Useful for debugging and monitoring live streams.

###  3Ô∏è‚É£ Broadcast Results

```bash
export function handleResultToStream(allBest) { ... }
```

Sends a JSON payload (allBest) to all connected clients.

####  Logs:

```bash
clients_count: Number of connected clients

payload_size: Size of payload in bytes
```

Sends a heartbeat comment (:heartbeat) to clients every 5 seconds to keep connections alive.


###  4Ô∏è‚É£ Register New Client

```bash
export function registerStream(req, res) { ... }
```

####  Sets SSE headers:

Content-Type: text/event-stream

Cache-Control: no-cache

Connection: keep-alive

Adds the client res to the clients array.

####  Logs:

client_connected with current client count

Handles client disconnects:

Removes disconnected clients

Logs client_disconnected event

###  5Ô∏è‚É£ Global Heartbeat

```bash
setInterval(() => {
  clients.forEach((res) => res.write(":heartbeat\n\n"));
  if (clients.length > 0) {
    logSSE("heartbeat", { clients_count: clients.length });
  }
}, 10000);
```

Sends a global heartbeat every 10 seconds to all clients.

Keeps SSE connections alive.

Logs heartbeat events only if clients are connected.

#### üìù Usage

Register a new client:

```bash
app.get("/stream", (req, res) => {
  registerStream(req, res);
});
```
Push results to clients:

```bash
handleResultToStream(allBestArray);
```

allBestArray is any array of results you want to broadcast.

---

# ‚òï Kafka Module (`/lib/kafka.js`)

This module manages **Kafka producers and consumers** for handling job dispatching, Puppeteer processing, and search results. It integrates **event-based logic** to retry failed jobs and ensures robust processing of messages.

---

## ‚ö° Key Features

### 1Ô∏è‚É£ Kafka Setup

```bash
const kafka = new Kafka({ 
  clientId: "scrapbackend", 
  brokers: ["localhost:9092"] 
});
```

####  Creates Kafka clients and producers:

producer: For general job dispatch

topProducer: For Puppeteer worker jobs

Consumers:

consumer: Handles /search and corporate results

puppeteerConsumer: Handles results from Puppeteer worker

Heartbeat interval and session timeout configured for stability.

###  2Ô∏è‚É£ Send Messages

```bash
export async function sendMessage(data) { ... }
```

Sends a message to the "fromscrap" topic (Puppeteer jobs).

Logs message delivery status in JSON style:

‚úÖ Success

‚ùå Error

Includes payload size and preview.

```bash
export async function sendToPuppeteerWorker(data) { ... }
```

Sends job to "topuppeteerworker" topic.

Uses EventBus to retry messages if a timeout occurs.

Logs every action in structured JSON.

###  3Ô∏è‚É£ Consumer: General Job Processing

```bash
export async function startConsumer() { ... }
```

Subscribes to "toscrap-results" topic.

####  Handles incoming messages:

Corporate results ‚Üí handleResult

Site results ‚Üí directToLLM

Links array ‚Üí split into 3 parts and sent to Puppeteer worker

Includes helper function splitIntoThreeParts to divide large payloads.

###  4Ô∏è‚É£ Consumer: Puppeteer Worker

```bash
export async function startPuppeteerConsumer() { ... }
```

Subscribes to "frompuppeteerworker" topic.

####  Buffers job results using puppeteerBuffer:

Tracks jobIds to avoid duplicates

Collects results until all expected jobs are received

####  Uses timeout logic (3 minutes) to resend incomplete jobs via EventBus:

Ensures no job is lost

Logs events with ‚úÖ, ‚ö†Ô∏è, üéØ for clarity

Calls handleResult when all expected jobs are collected.

###  5Ô∏è‚É£ EventBus Integration

```bash
eventBus.on("sending again", async (signal) => { ... });
```

Listens for resending signals from Puppeteer consumer timeout.

Ensures reliability in asynchronous job processing.

###  6Ô∏è‚É£ Utility Functions

```bash
function splitIntoThreeParts(array) { ... }
```

Splits an array into 3 non-overlapping chunks for efficient Puppeteer job distribution.

#### üîó Flow Diagram

```bash
/search POST ‚Üí Kafka Producer ‚Üí Puppeteer Worker
             ‚Üò Kafka Consumer ‚Üí handleResult / directToLLM
fromPuppeteerWorker ‚Üí PuppeteerConsumer ‚Üí buffer ‚Üí handleResult
EventBus ‚Üí retry messages on timeout
```

### üìù Logging

All actions are logged in JSON format for structured monitoring.

####  Includes:

Timestamp

Logger name

Event type

Payload size & preview

Errors (if any)


---


# üõ†Ô∏è Processor Module (`/lib/processor.js`)

This module handles **htmltext processing, chunking, vectorization, and LLM integration** for search results and corporate data.

---

## ‚ö° Key Features

### 1Ô∏è‚É£ Chunking

```js
export function cleanAndChunk(results) { ... }
```

####  Cleans raw HTML/text:

Removes URLs

Splits into lines and trims whitespace

Filters lines with at least 3 words

Splits content into smaller chunks of 7‚Äì12 words

Groups chunks into sets of 10 for vectorization

Preserves metadata for downstream processing


###  2Ô∏è‚É£ Vectorization

```bash
export async function vectorize(chunks) { ... }
export async function vectorizecorporate(chunks) { ... }
```

Sends text chunks to FastAPI embedding endpoints:

/embed ‚Üí general results

/embedcorporate ‚Üí corporate data

Logs request, response, and errors in JSON format

Emits statusChanged events for workflow synchronization


###  3Ô∏è‚É£ LLM Integration

```bash
export async function goingToLLM(htmlTextArray) { ... }
```

Uses Google Generative Language API (gemini-2.0-flash) to generate Q&A from site text

####  Returns clean JSON with:

question

options array

Attaches original site metadata

Handles malformed JSON gracefully


###  4Ô∏è‚É£ Vectorized Search

```bash
async function searchVectorDB(question, options, metadata) { ... }
```

Sends Q&A to vectorized database

Returns best answer and full ranking from vectorembedgen

Logs structured information:

Question

Best answer

URL of source


```bash
export async function directToLLM(result) { ... }
```

Converts result.resultssite to Q&A using LLM

Sends each question to vectorized search

Streams best answers to SSE clients via handleResultToStream

Structured logging of every processed Q&A


###  5Ô∏è‚É£ Corporate Vectorization

```bash
export async function handleResult(result) { ... }
```

Handles both internal corporate tables and external search results

Cleans and chunks data

Calls appropriate vectorization endpoint

Logs success or warnings for missing chunks

###  6Ô∏è‚É£ Utility Logging

logResultVariable(result) ‚Üí logs search result received from backend

logBestVariable(best) ‚Üí logs best vectorized search result

logVectorizedCorporate(vectorized) ‚Üí logs corporate vectorization

All logs are structured in JSON for monitoring and observability.


#### üîó Workflow

```bash
Raw results ‚Üí cleanAndChunk ‚Üí vectorize / vectorizecorporate ‚Üí handleResult
LLM processing ‚Üí goingToLLM ‚Üí searchVectorDB ‚Üí directToLLM ‚Üí SSE stream
```
